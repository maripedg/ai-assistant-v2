*** Begin Patch
*** Update File: backend/providers/oci/chat_model.py
@@
-from langchain_community.llms import OCIGenAI
-from core.ports.chat_model import ChatModelPort
-
-class OciChatModel(ChatModelPort):
-    def __init__(
-        self,
-        model_id: str,
-        endpoint: str,
-        compartment_id: str,
-        auth_file_location: str | None = None,
-        auth_profile: str | None = None,
-    ):
-        kwargs = {}
-        if auth_file_location:
-            kwargs["auth_file_location"] = auth_file_location
-        if auth_profile:
-            kwargs["auth_profile"] = auth_profile
-        self.llm = OCIGenAI(
-            model_id=model_id,
-            service_endpoint=endpoint,
-            compartment_id=compartment_id,
-            **kwargs,
-        )
-
-    def generate(self, prompt: str) -> str:
-        return self.llm.invoke(prompt).strip()
+import logging
+from typing import Any
+
+import oci
+from langchain_community.llms import OCIGenAI
+from oci.generative_ai_inference import GenerativeAiInferenceClient
+from oci.generative_ai_inference.models import (
+    CohereLlmInferenceRequest,
+    GenerateTextDetails,
+    OnDemandServingMode,
+)
+
+from core.ports.chat_model import ChatModelPort
+
+
+logger = logging.getLogger(__name__)
+
+
+class OciChatModel(ChatModelPort):
+    def __init__(
+        self,
+        model_id: str,
+        endpoint: str,
+        compartment_id: str,
+        auth_file_location: str | None = None,
+        auth_profile: str | None = None,
+        **gen_kwargs: Any,
+    ):
+        self._mode = "text"
+        self._generation_params = {
+            "max_tokens": gen_kwargs.get("max_tokens"),
+            "temperature": gen_kwargs.get("temperature"),
+            "top_k": gen_kwargs.get("top_k"),
+            "top_p": gen_kwargs.get("top_p"),
+            "frequency_penalty": gen_kwargs.get("frequency_penalty"),
+            "presence_penalty": gen_kwargs.get("presence_penalty"),
+        }
+
+        if model_id.startswith("ocid1."):
+            if not auth_file_location or not auth_profile:
+                raise ValueError("model OCID requires explicit auth_file_location and auth_profile")
+            self._mode = "oci-text"
+            self._compartment_id = compartment_id
+            self._model_id = model_id
+            try:
+                config = oci.config.from_file(
+                    file_location=auth_file_location,
+                    profile_name=auth_profile,
+                )
+            except Exception as exc:  # noqa: BLE001
+                raise RuntimeError("Failed to load OCI configuration for text model") from exc
+
+            try:
+                self._client = GenerativeAiInferenceClient(
+                    config=config,
+                    service_endpoint=endpoint,
+                    retry_strategy=oci.retry.DEFAULT_RETRY_STRATEGY,
+                )
+            except Exception as exc:  # noqa: BLE001
+                raise RuntimeError("Failed to initialize OCI Generative AI Inference client") from exc
+
+            logger.debug("Initialized OciChatModel in mode=%s (OCID)", self._mode)
+        else:
+            kwargs = {}
+            if auth_file_location:
+                kwargs["auth_file_location"] = auth_file_location
+            if auth_profile:
+                kwargs["auth_profile"] = auth_profile
+            self._llm = OCIGenAI(
+                model_id=model_id,
+                service_endpoint=endpoint,
+                compartment_id=compartment_id,
+                **kwargs,
+            )
+            logger.debug("Initialized OciChatModel in mode=%s (alias=%s)", self._mode, model_id)
+
+    def generate(self, prompt: str) -> str:
+        if self._mode == "oci-text":
+            request = CohereLlmInferenceRequest(prompt=prompt, runtime_type="COHERE")
+            if self._generation_params["max_tokens"] is not None:
+                request.max_tokens = self._generation_params["max_tokens"]
+            if self._generation_params["temperature"] is not None:
+                request.temperature = self._generation_params["temperature"]
+            if self._generation_params["top_k"] is not None:
+                request.top_k = self._generation_params["top_k"]
+            if self._generation_params["top_p"] is not None:
+                request.top_p = self._generation_params["top_p"]
+            if self._generation_params["frequency_penalty"] is not None:
+                request.frequency_penalty = self._generation_params["frequency_penalty"]
+            if self._generation_params["presence_penalty"] is not None:
+                request.presence_penalty = self._generation_params["presence_penalty"]
+
+            details = GenerateTextDetails(
+                compartment_id=self._compartment_id,
+                serving_mode=OnDemandServingMode(model_id=self._model_id),
+                inference_request=request,
+            )
+
+            try:
+                response = self._client.generate_text(details)
+            except Exception as exc:  # noqa: BLE001
+                raise RuntimeError("OCI text generation request failed") from exc
+
+            inference = getattr(response.data, "inference_response", None)
+            generated_texts = getattr(inference, "generated_texts", None)
+            if generated_texts:
+                return (generated_texts[0].text or "").strip()
+            return ""
+
+        return self._llm.invoke(prompt).strip()
*** End Patch
